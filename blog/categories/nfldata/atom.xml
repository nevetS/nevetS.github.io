<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Nfldata | Steve Kallestad]]></title>
  <link href="http://nevetS.github.io/blog/categories/nfldata/atom.xml" rel="self"/>
  <link href="http://nevetS.github.io/"/>
  <updated>2014-07-07T17:42:44-07:00</updated>
  <id>http://nevetS.github.io/</id>
  <author>
    <name><![CDATA[Steve Kallestad]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[NFL Data - Let's Start Scraping]]></title>
    <link href="http://nevetS.github.io/blog/2014/07/05/nfl-data-lets-start-scraping/"/>
    <updated>2014-07-05T20:22:01-07:00</updated>
    <id>http://nevetS.github.io/blog/2014/07/05/nfl-data-lets-start-scraping</id>
    <content type="html"><![CDATA[<p>We have two categories of pages to get &ndash; The weekly summaries which show some summary data of games played during each week, and the actual game pages.</p>

<p>The weekly summaries can all be grabbed with easily predictable URLs so that will be the starting point.</p>

<!-- more -->


<p>The URLs for these pages are <a href="http://www.nfl.com/schedules/2001/REG1">http://www.nfl.com/schedules/2001/REG1</a> through <a href="http://www.nfl.com/schedules/2013/REG17.">http://www.nfl.com/schedules/2013/REG17.</a></p>

<p>The important parts are the last two segments of the URL.  We want to iterate from 2001-2013.  For each of those years, we want to iterate from 1-17.</p>

<p>There are some problems that we can run into.  The nfl.com site could go down.  It could insert ads unexpectedly into our pages.  My internet service could go down at any time.  This is a rapid project, so while I&rsquo;m not going to ignore the potential for problems, I am also not going to over-think them.  If a problem occurs I will troubleshoot my way through it.</p>

<p>I know I&rsquo;m going to need a javascript capable scraping utility.  I&rsquo;m going to go with Selenium.  Selenium is a library used mainly for unit testing web applications.  I&rsquo;ll simply point it to each URL, grab our data, and put it in a data store.  We&rsquo;ll post-process that data later to get the next category of pages.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[NFL Data - Preliminary Modeling]]></title>
    <link href="http://nevetS.github.io/blog/2014/07/05/nfl-data-preliminary-modeling/"/>
    <updated>2014-07-05T18:45:21-07:00</updated>
    <id>http://nevetS.github.io/blog/2014/07/05/nfl-data-preliminary-modeling</id>
    <content type="html"><![CDATA[<p>Before I even start to think about collecting data, I have to figure out where to store it.</p>

<p>On a project like this, I prefer to actually store all of my source data rather than to parse it out on the fly.  I&rsquo;d hate to run a script for 15 hours only to find out later that the data didn&rsquo;t fit my expectations.</p>

<p><img src="/images/posts/20140705/preliminary-data-diagram.png" alt="Preliminary Data Diagram" /></p>

<!-- more -->


<p>The above diagram was drawn using <a href="http://yuml.me/diagram/nofunky/class/draw">yUML</a>:</p>

<pre><code>// Preliminary data diagram
[PageData|id:integer;url:string;timestamp:integer;status:string;category:string;body:text]-will eventually become*[note:Games]
[Games]++-1..*&gt;[Plays] 
</code></pre>

<p>I don&rsquo;t want to spend too much time thinking about details I just don&rsquo;t know yet.  I don&rsquo;t have a purpose for the data, my directive is to collect it.</p>

<p>What I know at this point is that I will want to know the URL, the timestamp for when I grabbed the data, and a status.  I know that I will be grabbing at least two different kinds of pages at this point, so I&rsquo;ve added a category.</p>

<p>Where will I store the data?  The two things that spring immediately to mind are <a href="http://www.mysql.com/">MySQL</a> and <a href="http://www.mongodb.org/">MongoDB</a>.  I have both databases handy.  MySQL will take some time to get moving on, but not much.  Mongo allows you to define the data model as you go.</p>

<p>At this point I know a little bit about these fields:</p>

<ul>
<li><strong>id</strong>: Required, Primary Key, AutoDefined</li>
<li><strong>url</strong>: url&rsquo;s can be long, but the general limit is 2048 characters.</li>
<li><strong>timestamp</strong>: I prefer to keep timestamps in epoch time.  It saves me from having to wonder about leap years, time zones, and daylight savings times between environments when migrating data.  It also saves me from dealing with environment specific idiosyncrasies like the fact that different databases have different formatting and syntax.</li>
<li><strong>status</strong>: Usually I would enumerate status out, but I prefer to move more quickly on this project and not have to worry about defining a fixed set of status values.</li>
<li><strong>category</strong>: I might want a many-to-many relationship of categories if this project were to survive long term, but for now storing the data in-row will work.</li>
<li><strong>body</strong>: The html.  This is long text data.  I can&rsquo;t make any assumptions here other than there will be a lot of text.</li>
</ul>


<p>I also know that I don&rsquo;t want to end up visiting the same page twice.  I could use a unique constraint.  I&rsquo;d rather not at this point.  I prefer to manage uniqueness in the workflow and allow my data store to be more fault tolerant at the moment.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sample Project - NFL Data]]></title>
    <link href="http://nevetS.github.io/blog/2014/07/05/sample-project-nfl-data/"/>
    <updated>2014-07-05T17:43:42-07:00</updated>
    <id>http://nevetS.github.io/blog/2014/07/05/sample-project-nfl-data</id>
    <content type="html"><![CDATA[<p>I decided to put together a small project to show a little bit of code and how I might approach a given problem.</p>

<p>For this project, I want to spend minimal time.  The simple directive is: collect a set of play-by-play information on NFL games so that it can be analyzed later.</p>

<!-- more -->


<p>The first question to ask is where can I get that information.  If I head over to NFL.com and click through a few things, I end up at <a href="http://www.nfl.com/gamecenter/2013092906/2013/REG4/steelers@vikings?icampaign=GC_schedule_rr#menu=gameinfo|contentId%3A0ap2000000253076&amp;tab=analyze&amp;analyze=playbyplay">this link</a>.  On that page, I see play by play information:</p>

<p><img src="/images/posts/20140705/nfl-play-by-play.jpg" alt="Play by Play" /></p>

<p>The URL itself gives me all the information I need to know in order to gather as many games as I want:</p>

<p>Broken down:</p>

<p><img src="/images/posts/20140705/nfl-url.png" alt="URL Breakdown" /></p>

<p>Unfortunately, I don&rsquo;t know the dates of all the NFL games, I don&rsquo;t know which teams played, and I don&rsquo;t know who was home and who was away.</p>

<p>I&rsquo;m going to have to find another way.  If I go back a few steps in <a href="http://www.nfl.com/schedules/2013/REG4">my navigation</a>, I get to a page that shows links to all of the games from that particular week, with navigation points to other weeks and other years.</p>

<p>That&rsquo;s perfect!  I know right off the bat that I can get all of the links I need.</p>

<p>There are roughly 15 games per week, 17 weeks per year, and play by play data goes back 12 years.  So that means I need to collect data from roughly 3072 pages to get all of the data I need. If I download a page every 10 seconds from the nfl.com servers it will take me about 8 &frac12; hours.  If I do it every 30 seconds, it will take me 25 &frac12; hours.  If I did it every 3 seconds, I could have it all done in 2 &frac12; hours, but that could be problematic in that I would be over-using nfl.com resources.</p>

<p>After a bit of inspection, the data that I&rsquo;m looking for is not contained in the HTML immediately.   When I do the scraping, I&rsquo;m going to have to use a javascript capable utility.</p>
]]></content>
  </entry>
  
</feed>
